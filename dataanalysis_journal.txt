Project Purpose: To ultimately find the most important factor in determining the probabilities of any real estate being sold as well as other extraneous
factors leading to the final sale price of a given piece of real estate. This project is intended for those who want insights into the real 
estate market for either personal or practical purposes. For example, one might have an occupation as a real-estate agent and wants to know 
the market's preferences based on a quantitative approach. 

Entries in this document will not be entered by date, rather by its relevancy to the files that comprise of this entire real estate analysis. TODO sections will either be deleted or have an
asterisk in front of it to signify that it is no longer relevant or it has been completed.

list-sold_ALLVERSIONS: Attempt to utilize api_calling to realtor.com to collect real estate data in the DC area using api_key 
generated by rapidAPI. The final attempt, list-sold_VERSIONUPDATE, successfully created an exported csv file which was read into
modeltest_one, the notebook for the trial data analysis. 
	
	Method: The data generated by the API was extremely messy. Some of the data was nestled into two or three layers of dicts and 
	lists. Therefore, I created dfs of messy data and exported them to Excel for manual cleaning. Then I read the cleaned data back
	into the notebook and appended it to the main dataframe. Once the main dataframe was completed with all the cleaned data, I 
	exported it to a csv file which was read into another notebook for the data analysis. 
 
modeltest_one.ipynb: A preliminary test to see if my machine learning analysis method can be used on the data I found. The 
results point to a favorable outcome for further testing and analysis with the current skills that I possess. I dropped columns that
contained string objects and manipulated simple string-based columns into ordinal values that represented a few different possibilities.
I then proceeded to enable my notebook with the Scikit-learn dependencies. Lastly, I loaded my function from my previous project and
changed it to target the current dataframe in the notebook. 

	Method: I read in the cleaned data csv and dropped all string-based columns that were difficult to manipulate. Then I changed and
	replaced remaining string data with ordinal values for the subsequent data analysis. 
	
	The data analysis technique I used involves running a two-part, user-defined function. The first part entails running the data through a 
	scikit-learn pipeline that preprocesses the data automatically. This preprocessing includes scaling, reducing dimensionality and testing 
	the data. The second part of the function runs multiple statistical tests simultaneously and then returns the test that yielded the most 
	accurate results. 
	
	Results: Results were much more than modest for the price and square foot columns which was expected due to the various 
	possibilities of those objects (accuracy ~2% to 4%). Results for the property type, beds and state code were excellent,
	however there is a concern that the possibilities were too simple and lack explanatory power (accuracy ~79% to ~81%).
	
	*TODO: Recreate the same modeltest for modeltest_two, except include dummy data for cities and see if the dummy data will work
	in the same df as the one in modeltest_one.
	
modeltest_two.ipynb: A secondary test to see if dummy variables could be included into modeltest_one.ipynb. The results were successful. There were
no noticable differences or mistakes in the analysis of the data again. 

	Method: I repeated the steps to generating the first analysis dataframe as in modeltest_one, except I did not drop the city column which was my target for changing into dummy variables.
	I used the get_dummies function and generated a dataframe of dummy variables which I then merged into the main df for the analysis. 
	
	Results: The results on prop_type are more accurate than when prop_type is used in modeltest_one. This is likely due to the introduction of the dummy variables since there are more 
	variables that can be compared against each other within the overall model. There is roughly a 12-15% increase (increase by .1) in accuracy between this test and modeltest_one. 
	
	*TODO: 
		-Determine whether the realtor API provides both the sale and listing dates, and if so, determine the difference between those dates
		-Create dummy variables on all other string-based columns and merge their subsequent dataframes into the main df 
		-Figure out how to bin data such as the price into smaller, more manageable groups

modeltest_three.ipynb: A tertiary test to determine if binning my data can yield even more accurate test results using the same algorithms as before. I completed 2/3 of my TODO objectives so far,
which were: discovering times the listings spent on the market and creating the dummy variables while merging them into the df. 

	Method:

	Results:

	TODO: